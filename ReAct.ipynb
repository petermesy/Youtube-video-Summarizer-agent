{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5374259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_14616\\3912277778.py:12: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  search = TavilySearchResults()\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.tools import tool\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "# Initialize the Tavily search tool\n",
    "search = TavilySearchResults()\n",
    "\n",
    "@tool\n",
    "def search_tool(query: str):\n",
    "    \"\"\"\n",
    "    Search the web for information using Tavily API.\n",
    "\n",
    "    :param query: The search query string\n",
    "    :return: Search results related to the query\n",
    "    \"\"\"\n",
    "    return search.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17fe4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd06b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0372fe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_14616\\2774954940.py:1: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  search_tool(\"What's the weather like in Tokyo today?\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'Weather for Tokyo, Japan - Time and Date',\n",
       "  'url': 'https://www.timeanddate.com/weather/japan/tokyo',\n",
       "  'content': 'timeanddate.com\\nFlag for Japan\\n\\n# Weather in Tokyo, Japan\\n\\nPassing clouds.\\n\\nFeels Like: 105 °F  \\nForecast: 95 / 78 °F  \\nWind: 8 mph ↑ from South\\n\\n|  |  |\\n| --- | --- |\\n| Location: | Tokyo Heliport |\\n| Current Time: | Aug 21, 2025 at 7:11:12 pm |\\n| Latest Report: | Aug 21, 2025 at 4:00 pm |\\n| Visibility: | N/A |\\n| Pressure: | 29.77 \"Hg |\\n| Humidity: | 63% |\\n| Dew Point: | 77 °F |\\n\\nLocation of Tokyo\\nLocation\\n\\n## Upcoming 5 hours [...] | Amount of Rain | 0.02\" | 0.00\" | 0.00\" | 0.02\" | 0.01\" | 0.00\" | 0.00\" |\\n| Amount of Snow | 0.00\" | 0.00\" | 0.00\" | 0.00\" | 0.00\" | 0.00\" | 0.00\" |\\n|  |  |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| \\\\ Updated Thursday, August 21, 2025 4:42:35 pm Tokyo time - Weather by CustomWeather, © 2025 | | | | | | | | [...] |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- |\\n| Now | 8:00 pm | 9:00 pm | 10:00 pm | 11:00 pm | 12:00 am |\\n|  |  |  |  |  |  |\\n| 91 °F | 86 °F | 85 °F | 83 °F | 83 °F | 82 °F |\\n\\nSee more hour-by-hour weather\\n\\n## Forecast for the next 48 hours',\n",
       "  'score': 0.9418739},\n",
       " {'title': 'Tokyo weather in August 2025 - Weather25.com',\n",
       "  'url': 'https://www.weather25.com/asia/japan/tokyo?page=month&month=August',\n",
       "  'content': 'Patchy rain possible\\nSunny\\nPartly cloudy\\nSunny\\nPartly cloudy\\nPartly cloudy\\nLight rain shower\\nPartly cloudy\\nSunny\\nPatchy rain possible\\nPatchy rain possible\\nPatchy rain possible\\nLight rain shower\\nPartly cloudy\\nModerate or heavy rain shower\\nPatchy rain possible\\nPatchy rain possible\\nPartly cloudy\\nPartly cloudy\\nPatchy rain possible\\nPatchy rain possible\\nPartly cloudy\\nSunny\\nSunny\\nSunny\\nPatchy rain possible\\nPatchy rain possible\\nSunny\\nSunny\\nLight drizzle\\nPartly cloudy [...] | 24 Sunny 34° /28° | 25 Sunny 35° /28° | 26 Patchy rain possible 34° /29° | 27 Patchy rain possible 34° /28° | 28 Sunny 32° /27° | 29 Sunny 34° /28° | 30 Light drizzle 34° /29° |\\n| 31 Partly cloudy 34° /28° |  |  |  |  |  |  | [...] weather25.com\\nSearch\\nweather in Japan\\nRemove from your favorite locations\\nAdd to my locations\\nShare\\nweather in Japan\\n\\n# Tokyo weather in August 2025\\n\\nPatchy rain possible\\nPartly cloudy\\nClear\\nClear\\nClear\\nPatchy rain possible\\nPatchy rain possible\\nClear\\nClear\\nLight drizzle\\nPartly cloudy\\nPartly cloudy\\nClear\\nPatchy light drizzle\\n\\n## The average weather in Tokyo in August',\n",
       "  'score': 0.8991304},\n",
       " {'title': 'Tokyo, Japan Weather Conditions | Weather Underground',\n",
       "  'url': 'https://www.wunderground.com/weather/jp/tokyo',\n",
       "  'content': \"# Tokyo, Tokyo Prefecture, Japan Weather Conditionsstar\\\\_ratehome\\n\\nicon\\n\\nThank you for reporting this station. We will review the data in question.\\n\\nYou are about to report this weather station for bad data. Please select the information that is incorrect.\\n\\nSee more\\n\\n(Reset Map)\\n\\nNo PWS\\n\\nReset Map, or Add PWS.\\n\\naccess\\\\_time 7:07 PM JST on August 21, 2025 (GMT +9) | Updated 10 seconds ago\\n\\nicon\\n\\nMostly Cloudy\\n\\nGusts 2 °mph\\n\\nTomorrow's temperature is forecast to be NEARLY THE SAME as today. [...] icon\\nicon\\nicon\\nicon\\nicon\\nicon\\nicon\\nicon\\nAccess Logo\\n\\nWe recognize our responsibility to use data and technology for good. We may use or share your data with our data vendors. Take control of your data.\\n\\nThe Weather Company Logo\\nThe Weather Channel Logo\\nWeather Underground Logo\\nStorm Radar Logo\\n\\n© The Weather Company, LLC 2025\",\n",
       "  'score': 0.8947989},\n",
       " {'title': 'Tokyo weather in August 2025 | Japan: How hot?',\n",
       "  'url': 'https://www.weather2travel.com/japan/tokyo/august/',\n",
       "  'content': \"weather2travel.com - travel deals for your holiday in the sun\\nClick to search\\n\\n# Tokyo weather in August 2025\\n\\nExpect  daytime maximum temperatures of 31°C in Tokyo, Japan in August and high heat and humidity based on long-term weather averages. There are 6 hours of sunshine per day on average with 16 days with some rainfall and typically 147 mm of rainfall in the month. [...] 31°C maximum daytime temperature in August in Tokyo\\n6 hours of sunshine per day (43% of daylight hours) in August in Tokyo\\n16 days with some rainfall in August in Tokyo\\n24°C minimum night-time temperature in August in Tokyo\\n13 hours of daylight per day in August in Tokyo\\nHigh heat & humidity in August in Tokyo\\n147 mm monthly rainfall in August in Tokyo\\nUV (maximum) index 10 (Very High) in August in Tokyo\\n26°C sea temperature in August in Tokyo\\n\\n### More about Tokyo [...] Seeking out the teahouses of Tokyo, Japan\\nSeeking out the teahouses of Tokyo, Japan\\nA first timer's guide to Japan's most beguiling big cities\\nA first timer's guide to Japan's most beguiling big cities\\n7 ways to sample Osaka\\n7 ways to sample Osaka\\n\\n### How hot is it in Tokyo in August?\\n\\nDaytime temperatures usually reach 31°C in Tokyo in August with high heat and humidity, falling to 24°C at night.\\n\\n### How sunny is it in Tokyo in August?\",\n",
       "  'score': 0.83290404},\n",
       " {'title': 'Tokyo Weather in August 2025, Scorching Hot Summer - Agate Travel',\n",
       "  'url': 'https://www.agatetravel.com/japan/tokyo/weather-in-august.html',\n",
       "  'content': 'AgateTravel\\nWeLiveToServe\\n\\n# Tokyo Weather in August\\n\\nSeason: Late Summer  \\nIn Short: Hot and torrid, possible month of typhoon and heavy rains [...] thunderstorms in August so please remember to take your umbrella while traveling to Tokyo. [...] Tokyo weather in August is torrid, at a highest temperature of 38°C (100.4°F) or so. With the sharply increasing temperature, the weather in August can sometimes make people insufferable. It is even hotter in the middle of this month than the early days and the temperature still keeps the tendency of getting higher. In such a climate, August offers the longest sunshine hours throughout the year. Although the number of rainy days is less than before, there is a possibility of typhoons and',\n",
       "  'score': 0.78294516}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tool(\"What's the weather like in Tokyo today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa8eaf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Weather for Addis Ababa, Ethiopia - Time and Date',\n",
       "  'url': 'https://www.timeanddate.com/weather/ethiopia/addis-ababa',\n",
       "  'content': 'timeanddate.com\\nFlag for Ethiopia\\n\\n# Weather in Addis Ababa, Ethiopia\\n\\nPartly sunny.\\n\\nFeels Like: 63 °F  \\nForecast: 69 / 54 °F  \\nWind: 14 mph ↑ from Southwest\\n\\n|  |  |\\n| --- | --- |\\n| Location: | Addis Ababa Airport |\\n| Current Time: | Aug 21, 2025 at 1:37:49 pm |\\n| Latest Report: | Aug 21, 2025 at 1:00 pm |\\n| Visibility: | N/A |\\n| Pressure: | 30.24 \"Hg (22.92 \"Hg at 2354m altitude) |\\n| Humidity: | 77% |\\n| Dew Point: | 55 °F |\\n\\nLocation of Addis Ababa\\nLocation\\n\\n## Upcoming 5 hours [...] | Amount of Rain | 0.09\" | 0.12\" | 0.06\" | 0.06\" | 0.06\" | 0.29\" | 0.67\" |\\n| Amount of Snow | 0.00\" | 0.00\" | 0.00\" | 0.00\" | 0.00\" | 0.00\" | 0.00\" |\\n|  |  |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| \\\\ Updated Thursday, August 21, 2025 10:42:35 am Addis Ababa time - Weather by CustomWeather, © 2025 | | | | | | | | [...] | Feels Like | 66 °F | 54 °F | 51 °F | 55 °F | 68 °F | 56 °F | 57 °F |\\n| Wind Speed | 16 mph | 13 mph | 15 mph | 11 mph | 9 mph | 12 mph | 3 mph |\\n| Wind Direction | W ↑ | W ↑ | W ↑ | W ↑ | WSW ↑ | SW ↑ | WNW ↑ |\\n| Humidity | 67% | 89% | 94% | 90% | 66% | 89% | 95% |\\n| Dew Point | 55 °F | 53 °F | 53 °F | 55 °F | 56 °F | 55 °F | 55 °F |\\n| Visibility | 8 mi | 5 mi | 3 mi | 3 mi | 8 mi | 3 mi | 2 mi |\\n| Probability of Precipitation | 42% | 61% | 49% | 36% | 44% | 56% | 70% |',\n",
       "  'score': 0.9469168},\n",
       " {'title': 'Addis Ababa, Ethiopia Weather Conditions',\n",
       "  'url': 'https://www.wunderground.com/weather/et/addis-ababa',\n",
       "  'content': \"# Addis Ababa, Ethiopia Weather Conditionsstar\\\\_ratehome\\n\\nicon\\n\\nThank you for reporting this station. We will review the data in question.\\n\\nYou are about to report this weather station for bad data. Please select the information that is incorrect.\\n\\nSee more\\n\\n(Reset Map)\\n\\nNo PWS\\n\\nReset Map, or Add PWS.\\n\\naccess\\\\_time 1:33 PM EAT on August 21, 2025 (GMT +3) | Updated just now\\n\\nicon\\n\\nMostly Cloudy\\n\\nGusts 27 °mph\\n\\nToday's temperature is forecast to be NEARLY THE SAME as yesterday.\",\n",
       "  'score': 0.9168945},\n",
       " {'title': 'Addis Ababa weather in August 2025 | Weather25.com',\n",
       "  'url': 'https://www.weather25.com/africa/ethiopia/addis-ababa?page=month&month=August',\n",
       "  'content': 'weather25.com\\nSearch\\nweather in Ethiopia\\nRemove from your favorite locations\\nAdd to my locations\\nShare\\nweather in Ethiopia\\n\\n# Addis Ababa weather in August 2025\\n\\nLight rain shower\\nLight rain shower\\nLight rain shower\\nLight rain shower\\nPatchy light rain with thunder\\nLight rain shower\\nPatchy light rain with thunder\\nLight rain shower\\nPatchy light rain with thunder\\nLight rain shower\\nLight rain shower\\nPatchy rain possible\\nModerate or heavy rain shower\\nLight rain shower [...] ## The average weather in Addis Ababa in August\\n\\nThe temperatures in Addis Ababa in August are usually low and can range between 11°C and 20°C.\\n\\nWith more than 22 rainy days, Addis Ababa is an incredibly wet during August. Be sure to pack your umbrella and bring along your rubber boots to stay dry. It’s going to be raining cats and dogs this month!\\n\\nOur weather forecast can give you a great sense of what weather to expect in Addis Ababa in August 2025. [...] | 24 Light rain shower 16° /12° | 25 Patchy light rain with thunder 18° /12° | 26 Light rain shower 18° /11° | 27 Patchy light rain with thunder 18° /12° | 28 Light rain shower 15° /11° | 29 Patchy light rain with thunder 18° /10° | 30 Light rain shower 18° /10° |\\n| 31 Light rain shower 18° /11° |  |  |  |  |  |  |',\n",
       "  'score': 0.9154545},\n",
       " {'title': 'Addis Ababa - Weather Forecast Maps | Ventusky',\n",
       "  'url': 'https://www.ventusky.com/addis-ababa',\n",
       "  'content': '# Ventusky: Weather Forecast Maps\\n\\nIsobars\\n\\n# Addis Ababa\\n\\n|  |  |\\n| --- | --- |\\n| overcast | 17 °C |\\n|  |\\n| Wind  22 km/h |\\n\\novercast\\n\\n|  |  |\\n| --- | --- |\\n| Humidity | 77 % |\\n| Clouds | 80 % |\\n| Cloud base | 731 m |\\n\\nWeather report from station Addis Ababa/bole, Distance: 8 km (13:00 2025/08/21)\\n\\n## Weather for the next 24 hours [...] | 00:00 | 03:00 | 06:00 | 09:00 | 12:00 | 15:00 | 18:00 | 21:00 |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| mostly cloudy 12 °C 0 mm 0 %  NW  4 km/h | overcast 11 °C 0 mm 10 %  NW  4 km/h | partly cloudy 12 °C 0 mm 10 %  NW  6 km/h | overcast 14 °C 0 mm 10 %  W  9 km/h | mostly cloudy 17 °C 0 mm 0 %  W  11 km/h | overcast 17 °C 0 mm 20 %  W  13 km/h | mixed with rain showers 15 °C 0.2 mm 70 %  W  9 km/h | overcast 14 °C 0 mm 40 %  W  3 km/h | [...] | overcast 15 °C 0 mm 20 %  W  20 km/h  Gust:  47 km/h | overcast 15 °C 0 mm 10 %  W  17 km/h  Gust:  47 km/h | overcast 15 °C 0 mm 10 %  W  17 km/h  Gust:  47 km/h | overcast 15 °C 0 mm 10 %  W  13 km/h  Gust:  43 km/h | overcast 14 °C 0 mm 10 %  W  9 km/h  Gust:  43 km/h | partly cloudy 14 °C 0 mm 0 %  W  5 km/h | mostly cloudy 13 °C 0 mm 10 %  W  5 km/h | mostly cloudy 13 °C 0 mm 10 %  W  5 km/h | mostly cloudy 12 °C 0 mm 0 %  NW  4 km/h | mostly cloudy 12 °C 0 mm 0 %  NW  6 km/h | mostly',\n",
       "  'score': 0.9035075},\n",
       " {'title': 'Ethiopia weather in August 2025 - Weather25.com',\n",
       "  'url': 'https://www.weather25.com/africa/ethiopia?page=month&month=August',\n",
       "  'content': 'weather25.com\\nSearch\\nweather in Ethiopia\\nRemove from your favorite locations\\nAdd to my locations\\nShare\\nweather in Ethiopia\\n\\n# Ethiopia weather in August 2025\\n\\nLight rain shower\\nLight rain shower\\nLight rain shower\\nLight rain shower\\nPatchy light rain with thunder\\nLight rain shower\\nPatchy light rain with thunder\\nLight rain shower\\nPatchy light rain with thunder\\nLight rain shower\\nLight rain shower\\nPatchy rain possible\\nModerate or heavy rain shower\\nLight rain shower [...] | 24 Light rain shower 16° /12° | 25 Patchy light rain with thunder 18° /12° | 26 Light rain shower 18° /11° | 27 Patchy light rain with thunder 18° /12° | 28 Light rain shower 15° /11° | 29 Patchy light rain with thunder 18° /10° | 30 Light rain shower 18° /10° |\\n| 31 Light rain shower 18° /11° |  |  |  |  |  |  | [...] | Sun | Mon | Tue | Wed | Thu | Fri | Sat |\\n| --- | --- | --- | --- | --- | --- | --- |\\n|  |  |  |  |  | 1 Light rain shower 18° /11° | 2 Light rain shower 19° /11° |\\n| 3 Patchy rain possible 18° /11° | 4 Light rain shower 18° /11° | 5 Patchy rain possible 18° /11° | 6 Light rain shower 19° /11° | 7 Light rain shower 19° /11° | 8 Moderate or heavy rain shower 19° /11° | 9 Light rain shower 20° /11° |',\n",
       "  'score': 0.83932644}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tool(\"What's the weather like in Addis Ababa today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e2739c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def recommend_clothing(weather: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a clothing recommendation based on the provided weather description.\n",
    "\n",
    "    This function examines the input string for specific keywords or temperature indicators \n",
    "    (e.g., \"snow\", \"freezing\", \"rain\", \"85°F\") to suggest appropriate attire. It handles \n",
    "    common weather conditions like snow, rain, heat, and cold by providing simple and practical \n",
    "    clothing advice.\n",
    "\n",
    "    :param weather: A brief description of the weather (e.g., \"Overcast, 64.9°F\")\n",
    "    :return: A string with clothing recommendations suitable for the weather\n",
    "    \"\"\"\n",
    "    weather = weather.lower()\n",
    "    if \"snow\" in weather or \"freezing\" in weather:\n",
    "        return \"Wear a heavy coat, gloves, and boots.\"\n",
    "    elif \"rain\" in weather or \"wet\" in weather:\n",
    "        return \"Bring a raincoat and waterproof shoes.\"\n",
    "    elif \"hot\" in weather or \"85\" in weather:\n",
    "        return \"T-shirt, shorts, and sunscreen recommended.\"\n",
    "    elif \"cold\" in weather or \"50\" in weather:\n",
    "        return \"Wear a warm jacket or sweater.\"\n",
    "    else:\n",
    "        return \"A light jacket should be fine.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ca08b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[search_tool,recommend_clothing]\n",
    "\n",
    "tools_by_name={ tool.name:tool for tool in tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a407c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage,SystemMessage\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are a helpful AI assistant that thinks step-by-step and uses tools when needed.\n",
    "\n",
    "When responding to queries:\n",
    "1. First, think about what information you need\n",
    "2. Use available tools if you need current data or specific capabilities  \n",
    "3. Provide clear, helpful responses based on your reasoning and any tool results\n",
    "\n",
    "Always explain your thinking process to help users understand your approach.\n",
    "\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"scratch_pad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4386d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configure API key (either from environment or directly here)\n",
    "genai.configure(api_key=\"AIzaSyA0a3ld-hCKxrsnTCYZA_aU8JJdENqhHSg\")\n",
    "\n",
    "# Create a model instance\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")  # or gemini-2.0-flash, gemini-1.5-pro, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c7bb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-google-genai\n",
      "  Using cached langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
      "  Using cached google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langchain-google-genai) (0.3.74)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langchain-google-genai) (2.11.7)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.40.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.4)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.4.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.24.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.3.1)\n",
      "Downloading langchain_google_genai-2.1.9-py3-none-any.whl (49 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.5/1.4 MB 240.6 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.5/1.4 MB 240.6 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.5/1.4 MB 240.6 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.5/1.4 MB 240.6 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.5/1.4 MB 240.6 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.5/1.4 MB 240.6 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.5/1.4 MB 240.6 kB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 201.2 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 201.2 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 201.2 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 201.2 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 201.2 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 201.2 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 201.2 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 201.2 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 1.0/1.4 MB 174.4 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.0/1.4 MB 174.4 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.0/1.4 MB 174.4 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.0/1.4 MB 174.4 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.0/1.4 MB 174.4 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 1.3/1.4 MB 177.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.3/1.4 MB 177.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 178.7 kB/s eta 0:00:00\n",
      "Installing collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
      "\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "  Attempting uninstall: google-ai-generativelanguage\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   ------------- -------------------------- 1/3 [google-ai-generativelanguage]\n",
      "   -------------------------- ------------- 2/3 [langchain-google-genai]\n",
      "   -------------------------- ------------- 2/3 [langchain-google-genai]\n",
      "   -------------------------- ------------- 2/3 [langchain-google-genai]\n",
      "   -------------------------- ------------- 2/3 [langchain-google-genai]\n",
      "   -------------------------- ------------- 2/3 [langchain-google-genai]\n",
      "   ---------------------------------------- 3/3 [langchain-google-genai]\n",
      "\n",
      "Successfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "693c2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the Gemini model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9272a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chain of language, strong and bright,\n",
      "LangChain links LLMs, day and night.\n",
      "From data deep, it pulls the thread,\n",
      "And weaves a story, deftly led.\n",
      "\n",
      "With agents bold and memory keen,\n",
      "It builds applications, sharp and clean.\n",
      "A framework grand, for tasks untold,\n",
      "LangChain's power, brave and bold.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Create a simple chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Invoke the chain with a question\n",
    "response = chain.invoke({\"question\": \"Write a short poem about LangChain.\"})\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b20dd409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Paris\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--63f1b784-1573-4c9d-b859-dc7ef15b6a58-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Paris'}, 'id': '79e1f457-a0d2-49b7-9ae1-915f575356c1', 'type': 'tool_call'}] usage_metadata={'input_tokens': 26, 'output_tokens': 7, 'total_tokens': 33, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# 1. Initialize the LangChain-wrapped Gemini model, not the base GenerativeModel.\n",
    "# The `bind_tools` method is available on this object.\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "# 2. Define your tool.\n",
    "@tool\n",
    "def get_current_weather(location: str) -> dict:\n",
    "    \"\"\"Returns the current weather for a given location.\"\"\"\n",
    "    # This is a mock function, you would add your real logic here.\n",
    "    return {\"temperature\": \"25C\", \"condition\": \"Sunny\"}\n",
    "\n",
    "tools = [get_current_weather]\n",
    "\n",
    "# 3. Now you can successfully bind the tools to the model.\n",
    "model_react = model.bind_tools(tools)\n",
    "\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "# 4. Create your prompt and chain.\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"What is the weather like in Paris?\")\n",
    "])\n",
    "chain = chat_prompt | model_react\n",
    "\n",
    "# 5. Invoke the chain.\n",
    "response = chain.invoke({})\n",
    "\n",
    "# 6. Print the response.\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfd83473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph\n",
      "  Downloading langgraph-0.6.6-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: langchain-core>=0.1 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langgraph) (0.3.74)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
      "  Using cached langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
      "  Using cached langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.2.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langgraph) (2.11.7)\n",
      "Collecting xxhash>=3.5.0 (from langgraph)\n",
      "  Using cached xxhash-3.5.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
      "  Using cached ormsgpack-1.10.0-cp313-cp313-win_amd64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (0.4.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.4)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\peter\\desktop\\next-ai\\venv\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
      "Downloading langgraph-0.6.6-py3-none-any.whl (153 kB)\n",
      "Using cached langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
      "Using cached langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading langgraph_sdk-0.2.2-py3-none-any.whl (52 kB)\n",
      "Using cached ormsgpack-1.10.0-cp313-cp313-win_amd64.whl (121 kB)\n",
      "Using cached xxhash-3.5.0-cp313-cp313-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
      "\n",
      "   ------ --------------------------------- 1/6 [ormsgpack]\n",
      "   ------------- -------------------------- 2/6 [langgraph-sdk]\n",
      "   ------------- -------------------------- 2/6 [langgraph-sdk]\n",
      "   ------------- -------------------------- 2/6 [langgraph-sdk]\n",
      "   -------------------- ------------------- 3/6 [langgraph-checkpoint]\n",
      "   -------------------- ------------------- 3/6 [langgraph-checkpoint]\n",
      "   -------------------- ------------------- 3/6 [langgraph-checkpoint]\n",
      "   -------------------- ------------------- 3/6 [langgraph-checkpoint]\n",
      "   -------------------- ------------------- 3/6 [langgraph-checkpoint]\n",
      "   -------------------------- ------------- 4/6 [langgraph-prebuilt]\n",
      "   -------------------------- ------------- 4/6 [langgraph-prebuilt]\n",
      "   -------------------------- ------------- 4/6 [langgraph-prebuilt]\n",
      "   -------------------------- ------------- 4/6 [langgraph-prebuilt]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   ---------------------------------------- 6/6 [langgraph]\n",
      "\n",
      "Successfully installed langgraph-0.6.6 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.2 ormsgpack-1.10.0 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a1cfe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (Annotated,Sequence,TypedDict)\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "\n",
    "    # add_messages is a reducer\n",
    "    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44f835c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After greeting: [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={})]\n",
      "After question: {'messages': [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}), HumanMessage(content='Weather in NYC?', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "# Example conversation flow:\n",
    "state: AgentState = {\"messages\": []}\n",
    "\n",
    "# append a message using the reducer properly\n",
    "state[\"messages\"] = add_messages(state[\"messages\"], [HumanMessage(content=\"Hi\")])\n",
    "print(\"After greeting:\", state[\"messages\"])\n",
    "\n",
    "# add another message (e.g. a question)\n",
    "state[\"messages\"] = add_messages(state[\"messages\"], [HumanMessage(content=\"Weather in NYC?\")])\n",
    "print(\"After question:\", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3887ad99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"What's the weather like in Zurich, and what should I wear based on the temperature?\", additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Zurich\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--c56d7157-4650-4bfe-ae9a-f73da837b500-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Zurich'}, 'id': '3477a175-678b-4b68-b33a-95b58b1cd868', 'type': 'tool_call'}], usage_metadata={'input_tokens': 37, 'output_tokens': 8, 'total_tokens': 45, 'input_token_details': {'cache_read': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "dummy_state: AgentState = {\n",
    "    \"messages\": [HumanMessage(\"What's the weather like in Zurich, and what should I wear based on the temperature?\")]\n",
    "}\n",
    "\n",
    "# Correctly pass the list of messages directly to invoke\n",
    "response = model_react.invoke(dummy_state[\"messages\"])\n",
    "\n",
    "# The rest of your code to process the response\n",
    "# Note: I am assuming `add_messages` is a function from your langgraph setup.\n",
    "dummy_state[\"messages\"] = add_messages(dummy_state[\"messages\"], [response])\n",
    "print(dummy_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84ae9170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call: {'name': 'get_current_weather', 'args': {'location': 'Zurich'}, 'id': '59aa2e61-801d-44ee-9c22-526e624e7545', 'type': 'tool_call'}\n",
      "Tool result preview: {'temperature': '25C', 'condition': 'Sunny'}\n",
      "{'messages': [HumanMessage(content=\"What's the weather like in Zurich, and what should I wear based on the temperature?\", additional_kwargs={}, response_metadata={}), ToolMessage(content='{\"temperature\": \"25C\", \"condition\": \"Sunny\"}', name='get_current_weather', tool_call_id='59aa2e61-801d-44ee-9c22-526e624e7545')]}\n"
     ]
    }
   ],
   "source": [
    "dummy_state: AgentState = {\n",
    "    \"messages\": [HumanMessage(\"What's the weather like in Zurich, and what should I wear based on the temperature?\")]\n",
    "}\n",
    "\n",
    "# Assume 'response' is the output from the previous step\n",
    "response = model_react.invoke(dummy_state[\"messages\"])\n",
    "\n",
    "tool_call = response.tool_calls[-1]\n",
    "print(\"Tool call:\", tool_call)\n",
    "\n",
    "# Now this will work\n",
    "tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "print(\"Tool result preview:\", tool_result)\n",
    "\n",
    "# Note: The `add_messages` function would be defined within your LangGraph setup\n",
    "# This example uses a simplified version\n",
    "def add_messages(existing_messages, new_messages):\n",
    "    existing_messages.extend(new_messages)\n",
    "    return existing_messages\n",
    "\n",
    "tool_message = ToolMessage(\n",
    "    content=json.dumps(tool_result),\n",
    "    name=tool_call[\"name\"],\n",
    "    tool_call_id=tool_call[\"id\"]\n",
    ")\n",
    "dummy_state[\"messages\"] = add_messages(dummy_state[\"messages\"], [tool_message])\n",
    "\n",
    "print(dummy_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47fa3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model_react.invoke(dummy_state[\"messages\"])\n",
    "\n",
    "dummy_state['messages'] = add_messages(dummy_state['messages'], [response])\n",
    "\n",
    "# check if the model wants to use another tool\n",
    "if response.tool_calls:\n",
    "    tool_call = response.tool_calls[0]\n",
    "    tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "    tool_message = ToolMessage(\n",
    "        content=json.dumps(tool_result),\n",
    "        name=tool_call[\"name\"],\n",
    "        tool_call_id=tool_call[\"id\"]\n",
    "    )\n",
    "    dummy_state['messages'] = add_messages(dummy_state['messages'], [tool_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37777278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final response generated: True\n",
      "More tools needed: False\n"
     ]
    }
   ],
   "source": [
    "response = model_react.invoke(dummy_state[\"messages\"])\n",
    "print(\"Final response generated:\", response.content is not None)\n",
    "print(\"More tools needed:\", bool(response.tool_calls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3087ec5",
   "metadata": {},
   "source": [
    "## Automating ReAct with Graphs\n",
    "\n",
    "### Why Use Graphs?\n",
    "\n",
    "Manual ReAct execution is educational but impractical for real applications. LangGraph automates this process with a state machine that handles the reasoning loop automatically.\n",
    "\n",
    "### Building the Core Functions\n",
    "\n",
    "#### Tool Execution Node\n",
    "Function Purpose:\n",
    "\n",
    "Automatically execute all tool calls from the model\n",
    "Handle multiple simultaneous tool calls\n",
    "Return properly formatted tool messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "366484df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_node(state: AgentState):\n",
    "    \"\"\"Execute all tool calls from the last message in the state.\"\"\"\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6332e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: AgentState):\n",
    "    print(\"\\n[DEBUG] call_model - messages:\", state[\"messages\"])\n",
    "    print(\"[DEBUG] call_model - types:\", [type(m) for m in state[\"messages\"]])\n",
    "    for i, m in enumerate(state[\"messages\"]):\n",
    "        print(f\"[DEBUG] call_model - message {i} content:\", getattr(m, 'content', m))\n",
    "    response = model_react.invoke(state[\"messages\"])\n",
    "    # Return the full message history, appending the new response\n",
    "    return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "def tool_node(state: AgentState):\n",
    "    print(\"\\n[DEBUG] tool_node - messages:\", state[\"messages\"])\n",
    "    print(\"[DEBUG] tool_node - types:\", [type(m) for m in state[\"messages\"]])\n",
    "    for i, m in enumerate(state[\"messages\"]):\n",
    "        print(f\"[DEBUG] tool_node - message {i} content:\", getattr(m, 'content', m))\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"]\n",
    "            )\n",
    "        )\n",
    "    # Return the full message history, appending all tool outputs\n",
    "    return {\"messages\": state[\"messages\"] + outputs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090a8b33",
   "metadata": {},
   "source": [
    "**Function Purpose:**\n",
    "- Implement the control flow logic\n",
    "- Decide whether the agent needs to use more tools\n",
    "- Route the conversation to either tool execution or completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "30584a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState):\n",
    "    \"\"\"Determine whether to continue with tool use or end the conversation.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b2f525",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48db3f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add edges between nodes\n",
    "workflow.add_edge(\"tools\", \"agent\")  # After tools, always go back to agent\n",
    "\n",
    "# Add conditional logic\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",  # If tools needed, go to tools node\n",
    "        \"end\": END,          # If done, end the conversation\n",
    "    },\n",
    ")\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d40dc67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "88bd3d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    \"\"\"Helper function for formatting the stream nicely.\"\"\"\n",
    "    for s in stream:\n",
    "        messages = s[\"messages\"]\n",
    "        if isinstance(messages, list):\n",
    "            for message in messages:\n",
    "                message.pretty_print()\n",
    "        else:\n",
    "            print(messages)\n",
    "    print_stream(stream)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1e5a534d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages being sent to Gemini: [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}), HumanMessage(content='Weather in NYC?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(\"Messages being sent to Gemini:\", state[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "97109cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"What's the weather like in Zurich, and what should I wear based on the temperature?\")]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "140372f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"...\")]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9f55753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: AgentState):\n",
    "    print(\"\\n[DEBUG] call_model - messages:\", state[\"messages\"])\n",
    "    print(\"[DEBUG] call_model - types:\", [type(m) for m in state[\"messages\"]])\n",
    "    for i, m in enumerate(state[\"messages\"]):\n",
    "        print(f\"[DEBUG] call_model - message {i} content:\", getattr(m, 'content', m))\n",
    "    response = model_react.invoke(state[\"messages\"])\n",
    "    # Return the full message history, appending the new response\n",
    "    return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "def tool_node(state: AgentState):\n",
    "    print(\"\\n[DEBUG] tool_node - messages:\", state[\"messages\"])\n",
    "    print(\"[DEBUG] tool_node - types:\", [type(m) for m in state[\"messages\"]])\n",
    "    for i, m in enumerate(state[\"messages\"]):\n",
    "        print(f\"[DEBUG] tool_node - message {i} content:\", getattr(m, 'content', m))\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"]\n",
    "            )\n",
    "        )\n",
    "    # Return the full message history, appending all tool outputs\n",
    "    return {\"messages\": state[\"messages\"] + outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "464a54a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'langchain_core.messages.human.HumanMessage'> Content: Hello, Gemini! What is the weather in Zurich?\n",
      "Gemini response: content='I\\'m sorry, I don\\'t have real-time access to weather information. To find out the weather in Zurich, I recommend checking a reliable weather app or website like:\\n\\n*   **Google Weather:** Just search \"weather Zurich\" on Google.\\n*   **AccuWeather:** (accuweather.com)\\n*   **The Weather Channel:** (weather.com)\\n*   **MeteoSwiss:** (meteoswiss.admin.ch) (Switzerland\\'s national weather service)\\n\\nThese sources will give you the most up-to-date and accurate forecast.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--e5488a37-f299-4bb5-bca5-8c879b509c4a-0' usage_metadata={'input_tokens': 11, 'output_tokens': 123, 'total_tokens': 134, 'input_token_details': {'cache_read': 0}}\n",
      "Gemini response: content='I\\'m sorry, I don\\'t have real-time access to weather information. To find out the weather in Zurich, I recommend checking a reliable weather app or website like:\\n\\n*   **Google Weather:** Just search \"weather Zurich\" on Google.\\n*   **AccuWeather:** (accuweather.com)\\n*   **The Weather Channel:** (weather.com)\\n*   **MeteoSwiss:** (meteoswiss.admin.ch) (Switzerland\\'s national weather service)\\n\\nThese sources will give you the most up-to-date and accurate forecast.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--e5488a37-f299-4bb5-bca5-8c879b509c4a-0' usage_metadata={'input_tokens': 11, 'output_tokens': 123, 'total_tokens': 134, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ea126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "msg = HumanMessage(content=\"Hello, Gemini! What is the weather in Zurich?\")\n",
    "print(\"Type:\", type(msg), \"Content:\", msg.content)\n",
    "response = model.invoke([msg])\n",
    "print(\"Gemini response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af2474bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's the weather like in Zurich, and what should I wear based on the temperature?\n",
      "\n",
      "[DEBUG] call_model - messages: [HumanMessage(content=\"What's the weather like in Zurich, and what should I wear based on the temperature?\", additional_kwargs={}, response_metadata={})]\n",
      "[DEBUG] call_model - types: [<class 'langchain_core.messages.human.HumanMessage'>]\n",
      "[DEBUG] call_model - message 0 content: What's the weather like in Zurich, and what should I wear based on the temperature?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_current_weather (15c8270f-83e5-4ce6-8209-bc923fe81e43)\n",
      " Call ID: 15c8270f-83e5-4ce6-8209-bc923fe81e43\n",
      "  Args:\n",
      "    location: Zurich\n",
      "\n",
      "[DEBUG] tool_node - messages: [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Zurich\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--d4648b4b-07e0-4b60-870f-a4294690fe8c-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Zurich'}, 'id': '15c8270f-83e5-4ce6-8209-bc923fe81e43', 'type': 'tool_call'}], usage_metadata={'input_tokens': 37, 'output_tokens': 8, 'total_tokens': 45, 'input_token_details': {'cache_read': 0}})]\n",
      "[DEBUG] tool_node - types: [<class 'langchain_core.messages.ai.AIMessage'>]\n",
      "[DEBUG] tool_node - message 0 content: \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_current_weather\n",
      "\n",
      "{\"temperature\": \"25C\", \"condition\": \"Sunny\"}\n",
      "\n",
      "[DEBUG] call_model - messages: [ToolMessage(content='{\"temperature\": \"25C\", \"condition\": \"Sunny\"}', name='get_current_weather', tool_call_id='15c8270f-83e5-4ce6-8209-bc923fe81e43')]\n",
      "[DEBUG] call_model - types: [<class 'langchain_core.messages.tool.ToolMessage'>]\n",
      "[DEBUG] call_model - message 0 content: {\"temperature\": \"25C\", \"condition\": \"Sunny\"}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_current_weather (15c8270f-83e5-4ce6-8209-bc923fe81e43)\n",
      " Call ID: 15c8270f-83e5-4ce6-8209-bc923fe81e43\n",
      "  Args:\n",
      "    location: Zurich\n",
      "\n",
      "[DEBUG] tool_node - messages: [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Zurich\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--d4648b4b-07e0-4b60-870f-a4294690fe8c-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Zurich'}, 'id': '15c8270f-83e5-4ce6-8209-bc923fe81e43', 'type': 'tool_call'}], usage_metadata={'input_tokens': 37, 'output_tokens': 8, 'total_tokens': 45, 'input_token_details': {'cache_read': 0}})]\n",
      "[DEBUG] tool_node - types: [<class 'langchain_core.messages.ai.AIMessage'>]\n",
      "[DEBUG] tool_node - message 0 content: \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_current_weather\n",
      "\n",
      "{\"temperature\": \"25C\", \"condition\": \"Sunny\"}\n",
      "\n",
      "[DEBUG] call_model - messages: [ToolMessage(content='{\"temperature\": \"25C\", \"condition\": \"Sunny\"}', name='get_current_weather', tool_call_id='15c8270f-83e5-4ce6-8209-bc923fe81e43')]\n",
      "[DEBUG] call_model - types: [<class 'langchain_core.messages.tool.ToolMessage'>]\n",
      "[DEBUG] call_model - message 0 content: {\"temperature\": \"25C\", \"condition\": \"Sunny\"}\n"
     ]
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents: contents is not specified\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:216\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:868\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    867\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mInvalidArgument\u001b[39m: 400 * GenerateContentRequest.contents: contents is not specified\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m             message.pretty_print()\n\u001b[32m     10\u001b[39m inputs = {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWhat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms the weather like in Zurich, and what should I wear based on the temperature?\u001b[39m\u001b[33m\"\u001b[39m)]}\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mprint_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mprint_stream\u001b[39m\u001b[34m(stream)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint_stream\u001b[39m(stream):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Helper function for formatting the stream nicely.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2647\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2646\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2657\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mcall_model\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[DEBUG] call_model - message \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m content:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m, m))\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43mmodel_react\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [response]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5441\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5434\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5435\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5436\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5439\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5440\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5442\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5443\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5444\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5445\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1488\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   1483\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1484\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1485\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1486\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1488\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:383\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    373\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    378\u001b[39m     **kwargs: Any,\n\u001b[32m    379\u001b[39m ) -> BaseMessage:\n\u001b[32m    380\u001b[39m     config = ensure_config(config)\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    382\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    393\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1006\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    997\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    999\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1003\u001b[39m     **kwargs: Any,\n\u001b[32m   1004\u001b[39m ) -> LLMResult:\n\u001b[32m   1005\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:825\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    823\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    824\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    831\u001b[39m         )\n\u001b[32m    832\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    833\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1072\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1070\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1076\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1595\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1568\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1569\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1570\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1581\u001b[39m     **kwargs: Any,\n\u001b[32m   1582\u001b[39m ) -> ChatResult:\n\u001b[32m   1583\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1584\u001b[39m         messages,\n\u001b[32m   1585\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1593\u001b[39m         **kwargs,\n\u001b[32m   1594\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1595\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1596\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1597\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1600\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1601\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:247\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    240\u001b[39m params = (\n\u001b[32m    241\u001b[39m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (request := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[32m    246\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda4\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda4\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\peter\\Desktop\\next-ai\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:227\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    228\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    229\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.ResourceExhausted \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    231\u001b[39m     \u001b[38;5;66;03m# Handle quota-exceeded error with recommended retry delay\u001b[39;00m\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mretry_after\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m e.retry_after < kwargs.get(\n\u001b[32m    233\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwait_exponential_max\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m60.0\u001b[39m\n\u001b[32m    234\u001b[39m     ):\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents: contents is not specified\n",
      "During task with name 'agent' and id 'bd87509a-3764-9fa9-9507-03d251dd2806'"
     ]
    }
   ],
   "source": [
    "def print_stream(stream):\n",
    "    \"\"\"Helper function for formatting the stream nicely.\"\"\"\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"What's the weather like in Zurich, and what should I wear based on the temperature?\")]}\n",
    "\n",
    "print_stream(graph.stream(inputs, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d8cf0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
